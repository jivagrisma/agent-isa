#!/usr/bin/env python3
"""
Script de prueba para el m√≥dulo de b√∫squeda web.
"""

import argparse
import asyncio
import json
import logging
import sys
from typing import List, Dict, Any, Optional, Union

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# A√±adir el directorio actual al path para importar m√≥dulos locales
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# Importar m√≥dulos necesarios
from modules.core import ConfigManager
from modules.search import WebSearchEngine, HeadlessBrowser, SearchResult

async def test_web_search(
    query: str,
    num_results: int,
    engine: str,
    language: Optional[str] = None,
    country: Optional[str] = None,
    filters: Optional[Dict[str, Any]] = None
):
    """
    Prueba la funcionalidad de b√∫squeda web.

    Args:
        query: Consulta de b√∫squeda
        num_results: N√∫mero de resultados a mostrar
        engine: Motor de b√∫squeda a utilizar
        language: C√≥digo de idioma (ej. "es", "en")
        country: C√≥digo de pa√≠s (ej. "es", "us")
        filters: Filtros adicionales para la b√∫squeda
    """
    try:
        # Inicializar gestor de configuraci√≥n
        config_manager = ConfigManager()

        # Inicializar motor de b√∫squeda
        search_engine = WebSearchEngine(config_manager)

        # Construir mensaje de b√∫squeda
        search_msg = f"\nüîç Buscando '{query}' en {engine}"
        if language:
            search_msg += f", idioma: {language}"
        if country:
            search_msg += f", pa√≠s: {country}"
        if filters:
            search_msg += f", filtros: {json.dumps(filters, ensure_ascii=False)}"
        print(search_msg + "...\n")

        # Realizar b√∫squeda
        results = search_engine.search(
            query=query,
            num_results=num_results,
            search_engine=engine,
            language=language,
            country=country,
            filters=filters
        )

        # Mostrar resultados
        if results:
            print(f"‚úÖ Se encontraron {len(results)} resultados:\n")
            for i, result in enumerate(results, 1):
                print(f"{i}. {result.title}")
                print(f"   URL: {result.url}")
                print(f"   {result.snippet[:100]}...")

                # Mostrar metadatos si est√°n disponibles
                if result.metadata:
                    metadata_str = ", ".join(f"{k}: {v}" for k, v in result.metadata.items() if v)
                    if metadata_str:
                        print(f"   Metadatos: {metadata_str}")

                print()
        else:
            print("‚ùå No se encontraron resultados.")

        return results

    except Exception as e:
        logger.error(f"Error en b√∫squeda web: {e}")
        print(f"‚ùå Error: {e}")
        return []

async def test_headless_browser(url: str):
    """
    Prueba la funcionalidad del navegador headless.

    Args:
        url: URL a navegar
    """
    try:
        # Inicializar gestor de configuraci√≥n
        config_manager = ConfigManager()

        # Inicializar navegador headless
        browser = HeadlessBrowser(config_manager)

        try:
            # Navegar a la URL
            print(f"\nüåê Navegando a {url}...\n")
            content = await browser.navigate(url)

            # Mostrar contenido
            print(f"‚úÖ Contenido extra√≠do ({len(content)} caracteres):\n")
            print(f"{content[:500]}...\n")

            return content

        finally:
            # Cerrar navegador
            await browser.close()

    except Exception as e:
        logger.error(f"Error en navegador headless: {e}")
        print(f"‚ùå Error: {e}")
        return ""

async def main():
    """
    Funci√≥n principal.
    """
    parser = argparse.ArgumentParser(description="Prueba del m√≥dulo de b√∫squeda web")
    subparsers = parser.add_subparsers(dest="command", help="Comando a ejecutar")

    # Comando: search
    search_parser = subparsers.add_parser("search", help="Realizar b√∫squeda web")
    search_parser.add_argument("query", help="Consulta de b√∫squeda")
    search_parser.add_argument("--results", type=int, default=5, help="N√∫mero de resultados")
    search_parser.add_argument("--engine", default="google", help="Motor de b√∫squeda")
    search_parser.add_argument("--lang", help="C√≥digo de idioma (ej. 'es', 'en')")
    search_parser.add_argument("--country", help="C√≥digo de pa√≠s (ej. 'es', 'us')")
    search_parser.add_argument("--site", help="Filtrar por sitio web")
    search_parser.add_argument("--filetype", help="Filtrar por tipo de archivo")
    search_parser.add_argument("--title", help="Filtrar por palabras en el t√≠tulo")
    search_parser.add_argument("--date", help="Filtrar por fecha (ej. 'd7' para √∫ltimos 7 d√≠as)")

    # Comando: browse
    browse_parser = subparsers.add_parser("browse", help="Navegar a una URL")
    browse_parser.add_argument("url", help="URL a navegar")

    # Comando: full
    full_parser = subparsers.add_parser("full", help="B√∫squeda y navegaci√≥n")
    full_parser.add_argument("query", help="Consulta de b√∫squeda")
    full_parser.add_argument("--results", type=int, default=1, help="N√∫mero de resultados a navegar")
    full_parser.add_argument("--engine", default="google", help="Motor de b√∫squeda")
    full_parser.add_argument("--lang", help="C√≥digo de idioma (ej. 'es', 'en')")
    full_parser.add_argument("--country", help="C√≥digo de pa√≠s (ej. 'es', 'us')")
    full_parser.add_argument("--site", help="Filtrar por sitio web")

    # Comando: advanced
    advanced_parser = subparsers.add_parser("advanced", help="B√∫squeda avanzada con extracci√≥n de contenido")
    advanced_parser.add_argument("query", help="Consulta de b√∫squeda")
    advanced_parser.add_argument("--results", type=int, default=3, help="N√∫mero de resultados")
    advanced_parser.add_argument("--engine", default="google", help="Motor de b√∫squeda")
    advanced_parser.add_argument("--lang", help="C√≥digo de idioma (ej. 'es', 'en')")
    advanced_parser.add_argument("--extract", choices=["text", "tables", "links", "all"], default="text",
                                help="Tipo de contenido a extraer")

    args = parser.parse_args()

    if args.command == "search":
        # Construir filtros
        filters = {}
        if args.site:
            filters["site"] = args.site
        if args.filetype:
            filters["file_type"] = args.filetype
        if args.title:
            filters["title_contains"] = args.title
        if args.date:
            filters["date_restrict"] = args.date

        # Realizar b√∫squeda
        await test_web_search(
            query=args.query,
            num_results=args.results,
            engine=args.engine,
            language=args.lang,
            country=args.country,
            filters=filters if filters else None
        )

    elif args.command == "browse":
        await test_headless_browser(args.url)

    elif args.command == "full":
        # Construir filtros
        filters = {}
        if args.site:
            filters["site"] = args.site

        # Realizar b√∫squeda
        results = await test_web_search(
            query=args.query,
            num_results=args.results,
            engine=args.engine,
            language=args.lang,
            country=args.country,
            filters=filters if filters else None
        )

        # Navegar al primer resultado
        if results:
            print(f"\nüîÑ Navegando al primer resultado...\n")
            await test_headless_browser(results[0].url)

    elif args.command == "advanced":
        # Realizar b√∫squeda
        results = await test_web_search(
            query=args.query,
            num_results=args.results,
            engine=args.engine,
            language=args.lang
        )

        # Importar extractor de contenido
        from modules.content import ContentExtractor
        content_extractor = ContentExtractor()

        # Navegar y extraer contenido de los resultados
        if results:
            browser = HeadlessBrowser()
            try:
                for i, result in enumerate(results, 1):
                    print(f"\nüîÑ Procesando resultado {i}: {result.title}...\n")

                    # Navegar a la URL
                    content = await browser.navigate(result.url)

                    # Extraer contenido seg√∫n el tipo seleccionado
                    extracted = content_extractor.extract_from_html(content, result.url)

                    # Mostrar contenido extra√≠do seg√∫n el tipo
                    if args.extract == "text" or args.extract == "all":
                        print(f"\nüìÑ Contenido principal ({len(extracted['main_content'])} caracteres):")
                        print(f"{extracted['main_content'][:500]}...\n")

                    if (args.extract == "tables" or args.extract == "all") and extracted['tables']:
                        print(f"\nüìä Tablas encontradas: {len(extracted['tables'])}")
                        for j, table in enumerate(extracted['tables'][:2], 1):
                            print(f"  Tabla {j}: {table['title']} ({table['num_rows']} filas, {table['num_cols']} columnas)")

                    if (args.extract == "links" or args.extract == "all") and extracted['links']:
                        print(f"\nüîó Enlaces encontrados: {len(extracted['links'])}")
                        for j, link in enumerate(extracted['links'][:5], 1):
                            print(f"  {j}. {link['text'][:50]}: {link['url']}")

                        if len(extracted['links']) > 5:
                            print(f"  ... y {len(extracted['links']) - 5} enlaces m√°s")
            finally:
                await browser.close()
        else:
            print("‚ùå No se encontraron resultados para extraer contenido.")

    else:
        parser.print_help()

if __name__ == "__main__":
    asyncio.run(main())
